---
title: "R Tutorial"
author: "David Smith"
date: "10/14/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
In this tutorial...

## Configure your environment

To use the Azure ML R SDK, you will need to have Conda installed along with Python 3.5. Follow the [installation instructions](https://azure.github.io/azureml-sdk-for-r/articles/installation.html).

```{r install, eval=FALSE}
remotes::install_github('https://github.com/Azure/azureml-sdk-for-r', INSTALL_opts=c("--no-multiarch"))

azureml::install_azureml()
```

## Launch RStudio and load the azureml package

We recommend using RStudio to run these examples. Launch RStudio and then:

```{r launch}
library(azureml)
```

We will also use some additional R packages:

```{r packages, eval=FALSE}
install.packages("DAAG")
install.packages("caret")
```


## Create a Workspace

An Azure Machine Learning workspace is a foundational resource in the cloud that you use to experiment, train, and deploy machine learning models. It ties your Azure subscription and resource group to an easily consumed object in the service.

You will need several bits of Azure information to proceed, which you 
can find by logging into the [Azure Portal](https://portal.azure.com). If you don't have an Azure subscription, create one by visiting https://aka.ms/azure-free-credits .

* Your Azure Subscription ID. Find this under "Subscriptions". It will look something like: 9178352c-d995-46ca-9e70-724ebd271024
* A resource group name. You can choose any name you like, here we create a new one called `aml-r-tutorial`.
* The Azure region location. Supported regions include: westus2 and ... ?

```{r create_ws}
## Provide your subscription ID here:
subscription_id <- Sys.getenv("SUBSCRIPTION_ID", unset = "<my-subscription-id>")

ws <- create_workspace(name = "r-analysis", 
                           subscription_id = subscription_id, 
			                     resource_group = "aml-r-tutorial", 
			                     location = "westus2", 
			                     create_resource_group = TRUE,
			                     exist_ok = TRUE)
```

Creating a workspace can take several minutes, but you only need to do it once. After the workspace is created, you can save it to a configuration file to the local machine.

```{r save_config}
write_workspace_config(ws)
## You can retrieve the workspace later with:
## ws <- load_workspace_from_config()

```

## Load data and prepare for training 

You can find other datasets to work with [here](https://vincentarelbundock.github.io/Rdatasets/datasets.html).

We will use data from the [DAAG package](https://cran.r-project.org/package=DAAG). This dataset includes data from over 25,000 car crashes in the US, with variables we can use to predict the likelihood of a fatality. First, let's import the data into R and transform it into a new dataframe `accidents` for analysis, and export it to an Rdata file.

```{r load_data}
library(DAAG)
data(nassCDS)

accidents <- na.omit(nassCDS[,c("dead","dvcat","seatbelt","frontal","sex","ageOFocc","yearVeh","airbag","occRole")])
accidents$frontal <- factor(accidents$frontal, labels=c("notfrontal","frontal"))
accidents$occRole <- factor(accidents$occRole)

saveRDS(accidents, file="accidents.Rd")
```
## Upload the data to the datastore
Azure ML workspaces provide a default datastore which you can use to store data and other files needed for analysis.
Here, we upload the accidents data we created above to the datastore.

```{r upload_data}
ds <- get_default_datastore(ws)
target_path <- "accidentdata"
upload_files_to_datastore(ds,
                          list("./accidents.Rd"),
                          target_path = target_path,
                          overwrite = TRUE)
```

## Creating a compute resource
When you need more power than your local laptop to train a model, create a compute resource. 
Here we will create a virtual machine in Azure (which we will call `rcluster`) to use for training our model.

```{r create_cluster}
cluster_name <- "rcluster"
compute_target <- get_compute(ws, cluster_name = cluster_name)
if (is.null(compute_target)) {
  vm_size <- "STANDARD_D2_V2" 
  compute_target <- create_aml_compute(workspace = ws,
                                       cluster_name = cluster_name,
                                       vm_size = vm_size,
                                       max_nodes = 1)
}
```

## Training a model

Now we can fit a logistic regression model on our uploaded data using our remote compute target. The script to fit the model will be called `accidents.R`, and it will be run as a command-line script with `Rscript` which takes one argument `-d` specifying the storage folder where the data file will be located. This argument will be provided for you by Azure ML Service.

We specify the script file name and other options by creating an `estimator` object, and we initiate the computation by submitting an `experiment`. Note in particular the `cran_packages` option, which defines the packages that will be installed on the compute cluster for R to use.

```{r train}
est <- estimator(source_directory = ".",
                 entry_script = "accidents.R",
                 script_params = list("--data_folder" = ds$path(target_path)),
                 compute_target = compute_target,
                 cran_packages = c("caret", "optparse", "e1071")
                 )

experiment_name <- "accident-logreg"
exp <- experiment(ws, experiment_name)

run <- submit_experiment(exp, est)
view_run_details(run)
wait_for_run_completion(run, show_output = TRUE)
```

In the file `accidents.R` we stored a metric from our model: the accuracy of the predictions in the training data. (You can store any metrics you like.)
You can see metrics in the Azure portal, or extract them to the local session as an R list as follows:

```{r metricss}
metrics <- get_run_metrics(run)
metrics
```

## Retrieving the model

Now that you've fit the logistic regression model in the remote compute resource, we can retrieve the model object and look at the results in our local R session. Here we see that impact speed, whether the driver was male, the age of the occupant, and whether the victim was a passenger significantly increase the estimated probability of death, while the presence of airbags and seatbelts, and whether the impact was a frontal collision decrease the probabilty. The vehicle year of manufacture does not have a significant effect.

```{r retrieve-model}
download_files_from_run(run, prefix="outputs/")
model <- readRDS("outputs/model.rds")
summary(model)
```

## Deploying a prediction endpoint

With our model, we can predict predict the danger death from of other types of collisions. We can use Azure ML to deploy our model as a prediction service, and then call the model from a Shiny app.



## Using the prediction endpoint from a Shiny app


## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
