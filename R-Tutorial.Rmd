---
title: "R Tutorial"
author: "David Smith"
date: "10/14/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
In this tutorial...

## Configure your environment

To use the Azure ML R SDK, you will need to have Conda installed along with Python 3.5. Follow the [installation instructions](https://azure.github.io/azureml-sdk-for-r/articles/installation.html).

```{r install, eval=FALSE}
remotes::install_github('https://github.com/Azure/azureml-sdk-for-r', INSTALL_opts=c("--no-multiarch"))

azureml::install_azureml()
```

## Launch RStudio and load the azureml package

We recommend using RStudio to run these examples. Launch RStudio and then:

```{r launch}
library(azureml)
```

We will also use some additional R packages:

```{r packages, eval=FALSE}
install.packages("DAAG")
install.packages("caret")
```


## Create a Workspace

An Azure Machine Learning workspace is a foundational resource in the cloud that you use to experiment, train, and deploy machine learning models. It ties your Azure subscription and resource group to an easily consumed object in the service.

You will need several bits of Azure information to proceed, which you 
can find by logging into the [Azure Portal](https://portal.azure.com). If you don't have an Azure subscription, create one by visiting https://aka.ms/azure-free-credits .

* Your Azure Subscription ID. Find this under "Subscriptions". It will look something like: 9178352c-d995-46ca-9e70-724ebd271024
* A resource group name. You can choose any name you like, here we create a new one called `aml-r-tutorial`.
* The Azure region location. Supported regions include: westus2 and ... ?

```{r create_ws}
## Provide your subscription ID here:
subscription_id <- Sys.getenv("SUBSCRIPTION_ID", unset = "<my-subscription-id>")

ws <- create_workspace(name = "r-analysis", 
                           subscription_id = subscription_id, 
			                     resource_group = "aml-r-tutorial", 
			                     location = "westus2", 
			                     create_resource_group = TRUE,
			                     exist_ok = TRUE)
```

Creating a workspace can take several minutes, but you only need to do it once. After the workspace is created, you can save it to a configuration file to the local machine.

```{r save_config}
write_workspace_config(ws)
## You can retrieve the workspace later with:
## ws <- load_workspace_from_config()

```

## Load data and prepare for training 

You can find other datasets to work with [here](https://vincentarelbundock.github.io/Rdatasets/datasets.html).

We will use data from the [DAAG package](https://cran.r-project.org/package=DAAG). This dataset includes data from over 25,000 car crashes in the US, with variables we can use to predict the likelihood of a fatality. First, let's import the data into R and transform it into a new dataframe `accidents` for analysis, and export it to an Rdata file.

```{r load_data}
library(DAAG)
data(nassCDS)

accidents <- na.omit(nassCDS[,c("dead","dvcat","seatbelt","frontal","sex","ageOFocc","yearVeh","airbag","occRole")])
accidents$frontal <- factor(accidents$frontal, labels=c("notfrontal","frontal"))
accidents$occRole <- factor(accidents$occRole)

saveRDS(accidents, file="accidents.Rd")
```
## Upload the data to the datastore
Azure ML workspaces provide a default datastore which you can use to store data and other files needed for analysis.
Here, we upload the accidents data we created above to the datastore.

```{r upload_data}
ds <- get_default_datastore(ws)
target_path <- "accidentdata"
upload_files_to_datastore(ds,
                          list("./accidents.Rd"),
                          target_path = target_path,
                          overwrite = TRUE)
```

## Creating a compute resource
When you need more power than your local laptop to train a model, create a compute resource. 
Here we will create a virtual machine in Azure (which we will call `rcluster`) to use for training our model.

```{r create_cluster}
cluster_name <- "rcluster"
compute_target <- get_compute(ws, cluster_name = cluster_name)
if (is.null(compute_target)) {
  vm_size <- "STANDARD_D2_V2" 
  compute_target <- create_aml_compute(workspace = ws,
                                       cluster_name = cluster_name,
                                       vm_size = vm_size,
                                       max_nodes = 1)
}
```

## Training a model

Now we can fit a logistic regression model on our uploaded data using our remote compute target. The script to fit the model will be called `accidents.R`, and it will be run as a command-line script with `Rscript` which takes one argument `-d` specifying the storage folder where the data file will be located. This argument will be provided for you by Azure ML Service.

We specify the script file name and other options by creating an `estimator` object, and we initiate the computation by submitting an `experiment`. Note in particular the `cran_packages` option, which defines the packages that will be installed on the compute cluster for R to use.

```{r train}
est <- estimator(source_directory = ".",
                 entry_script = "accidents.R",
                 script_params = list("--data_folder" = ds$path(target_path)),
                 compute_target = compute_target,
                 cran_packages = c("caret", "optparse", "e1071")
                 )

experiment_name <- "accident-logreg"
exp <- experiment(ws, experiment_name)

run <- submit_experiment(exp, est)
view_run_details(run)
wait_for_run_completion(run, show_output = TRUE)
```

(Note that you -- and colleagues with access to the workspace -- can submit multiple experiments in parallel, and Azure ML Services will take of scheduling the tasks on the compute cluster. You can even configure the cluster to automatically scale up to multiple nodes, and scale back when there are no more compute tasks in the queue. This is a cost-effective way for teams to share compute resources.)

In the file `accidents.R` we stored a metric from our model: the accuracy of the predictions in the training data. (You can store any metrics you like.)
You can see metrics in the Azure portal, or extract them to the local session as an R list as follows:

```{r metricss}
metrics <- get_run_metrics(run)
metrics
```

Note: if you've run multiple experiments (say, using differing variables, algorithms, or hyperparamers), you can use the metrics you've saved from each run to choose the model you will use in production.

## Retrieving the model

Now that you've fit the logistic regression model in the remote compute resource, we can retrieve the model object and look at the results in our local R session. Here we see that impact speed, whether the driver was male, the age of the occupant, and whether the victim was a passenger significantly increase the estimated probability of death, while the presence of airbags and seatbelts, and whether the impact was a frontal collision decrease the probability. The vehicle year of manufacture does not have a significant effect.

```{r retrieve-model}
download_files_from_run(run, prefix="outputs/")
accident_model <- readRDS("outputs/model.rds")
summary(accident_model)
```

You can use this model to make new predictions:

```{r manual_predict}
newdata <- data.frame( # valid values shown below
 dvcat="10-24",        # "1-9km/h" "10-24"   "25-39"   "40-54"   "55+"  
 seatbelt="none",      # "none"   "belted"  
 frontal="frontal",    # "notfrontal" "frontal"
 sex="f",              # "f" "m"
 ageOFocc=16,          # age in years, 16-97
 yearVeh=2002,         # year of vehicle, 1955-2003
 airbag="none",        # "none"   "airbag"   
 occRole="pass"        # "driver" "pass"
 )
## predicted probability of death for these variables, as a percentage
as.numeric(predict(accident_model,newdata, type="response")*100)
```

## Deploying a prediction endpoint

With our model, we can predict predict the danger death from of other types of collisions. We can use Azure ML to deploy our model as a prediction service, and then call the model from a Shiny app.

First, register the model you downloaded for deployment:

```
model <- register_model(ws, 
                        model_path = "outputs/model.rds", 
                        model_name = "accidents_model",
                        description = "Predict probablity of auto accident")
```

A registered model can be any collection of files, but in this case the R model object is sufficient. Azure ML Service will track models each time they are deployed, which is our next step.

To create a web service for your model, you first need to create an **entry script**: an R script which will take as input variable values (in JSON format) and output a prediction from your model. Use the file `accident-predict.R`.

Next, define an **environment** for your deployed model. With an environment, you can specify R packages (from CRAN or elsewhere) that need to be available for your entry script to run, and the values of environment variables (which your script can reference to modify its behavior). You can also specify a custom Docker image to use, if you need software other than R to be available. In our case we have no special requirements, so create an environment with no special attributes:

```{r Create environment, eval=FALSE}
r_env <- r_environment(name = "basic_env")
```

Now we have everything we need to create an **inference config** combining the  

``` {r Create inference config, eval=FALSE}
inference_config <- inference_config(
  entry_script = "accident-predict.R",
  source_directory = ".",
  environment = r_env)
```

We will deploy our service to Azure Container Instances. This will provision a single container to respond to inbound requests, which is suitable for testing and light loads. (For production scale, you can also deploy to Azure Kubernetes Service.)

``` {r Web service configuration, eval=FALSE}
aci_config <- aci_webservice_deployment_config(cpu_cores = 1, memory_gb = 0.5)
```

Now we can deploy our service. (Note that this can take several minutes.)

```{r Deploy web service to AKS, eval=FALSE}
aci_service <- deploy_model(ws, 
                        'accident-pred', 
                        list(model), 
                        inference_config, 
                        aci_config)
wait_for_deployment(aci_service, show_output = TRUE)
```

## Testing the model from R

Now that our model is deployed as a service, we can test the service from R.  Provide a new set of data to predict from, convert it to JSON, and send it to the service.

```{r test_deployment}
newdata <- data.frame( # valid values shown below
 dvcat="10-24",        # "1-9km/h" "10-24"   "25-39"   "40-54"   "55+"  
 seatbelt="none",      # "none"   "belted"  
 frontal="frontal",    # "notfrontal" "frontal"
 sex="f",              # "f" "m"
 ageOFocc=22,          # age in years, 16-97
 yearVeh=2002,         # year of vehicle, 1955-2003
 airbag="none",        # "none"   "airbag"   
 occRole="pass"        # "driver" "pass"
 )
prob <- invoke_webservice(aci_service, toJSON(newdata))
prob
```

## Using the prediction endpoint from a Shiny app


## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
